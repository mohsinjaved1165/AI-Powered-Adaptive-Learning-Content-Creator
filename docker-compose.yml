services:
  ollama:
    image: ollama/ollama:0.11.11
    container_name: adaptive-learning-ollama
    ports:
      - "11434:11434"
    environment:
      OLLAMA_CONTEXT_LENGTH: 4096
      OLLAMA_DEBUG: INFO
    command: ["serve"]
    restart: unless-stopped
    volumes:
      - ~/.ollama/models:/root/.ollama/models   # mount host models

  flask-app:
    build: .
    container_name: adaptive-learning-flask-app
    ports:
      - "8080:5000"   # use http://localhost:8080
    depends_on:
      - ollama
    environment:
      OLLAMA_HOST: http://ollama:11434
    volumes:
      - ./data:/app/data   # persist uploaded/sample docs
      - ./templates:/app/templates   # make sure index.html is always available
    command: >
      sh -c "python wait_for_ollama.py && python app.py"
    restart: unless-stopped
